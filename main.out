\BOOKMARK [1][-]{section.1}{Conference\040Highlights}{}% 1
\BOOKMARK [1][-]{section.2}{Sunday\040June\04016th:\040Tutorials\040&\040Workshops}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{Workshop:\040Deep-Vision}{section.2}% 3
\BOOKMARK [3][-]{subsubsection.2.1.1}{Topic:\040AI\040on\040Medicine,\040Speaker:\040Serena\040Yeung}{subsection.2.1}% 4
\BOOKMARK [3][-]{subsubsection.2.1.2}{Topic: Video Re-Id, Speaker: Prof. Dr. Laura Leal-Taixé}{subsection.2.1}% 5
\BOOKMARK [3][-]{subsubsection.2.1.3}{Topic: Video Super-resolution, Speaker: Prof. Dr. Laura Leal-Taixé}{subsection.2.1}% 6
\BOOKMARK [3][-]{subsubsection.2.1.4}{Topic:\040Google\040Brain\040Pierre\040Sermanet:\040Self-Supervision\040and\040Play}{subsection.2.1}% 7
\BOOKMARK [1][-]{section.3}{Monday\040June\04017th:\040Tutorials\040&\040Workshops}{}% 8
\BOOKMARK [2][-]{subsection.3.1}{Workshop:\040Computational\040Photography}{section.3}% 9
\BOOKMARK [3][-]{subsubsection.3.1.1}{Professor\040Peyman\040Milanfar:\040Computation\040+\040Photography\040How\040the\040mobile\040phone\040became\040a\040camera}{subsection.3.1}% 10
\BOOKMARK [3][-]{subsubsection.3.1.2}{SuperSR}{subsection.3.1}% 11
\BOOKMARK [3][-]{subsubsection.3.1.3}{Denoising}{subsection.3.1}% 12
\BOOKMARK [3][-]{subsubsection.3.1.4}{Style-based\040GAN:\040Kerras}{subsection.3.1}% 13
\BOOKMARK [3][-]{subsubsection.3.1.5}{Image\040Coloring\040Challenge}{subsection.3.1}% 14
\BOOKMARK [3][-]{subsubsection.3.1.6}{Opportunity\040Chiuman\040Ho,\040Director\040of\040AI:\040Imaging\040in\040the\040Dark}{subsection.3.1}% 15
\BOOKMARK [3][-]{subsubsection.3.1.7}{Blind\040Deconvolution:\040Professor\040Paolo\040Favaro}{subsection.3.1}% 16
\BOOKMARK [3][-]{subsubsection.3.1.8}{EDVR}{subsection.3.1}% 17
\BOOKMARK [3][-]{subsubsection.3.1.9}{Towards\040Versatile\040Image\040Restoration:\040Chen-Change\040LOY}{subsection.3.1}% 18
\BOOKMARK [1][-]{section.4}{Tuesday\040June\04018th:\040Main\040Conference\040Day\0401}{}% 19
\BOOKMARK [2][-]{subsection.4.1}{CVPR\040Main\040Day\0401}{section.4}% 20
\BOOKMARK [3][-]{subsubsection.4.1.1}{GNN}{subsection.4.1}% 21
\BOOKMARK [3][-]{subsubsection.4.1.2}{Kervolutional\040Neural\040Network}{subsection.4.1}% 22
\BOOKMARK [3][-]{subsubsection.4.1.3}{Relu\040with\040high\040confident}{subsection.4.1}% 23
\BOOKMARK [3][-]{subsubsection.4.1.4}{Structural\040Sensitivity\040of\040DCN\040to\040the\040Directions\040of\040FOurier\040Basis\040Functions}{subsection.4.1}% 24
\BOOKMARK [3][-]{subsubsection.4.1.5}{neural\040rejuvenation}{subsection.4.1}% 25
\BOOKMARK [3][-]{subsubsection.4.1.6}{On\040the\040Structural\040Sensitivity\040of\040Deep\040Convolution}{subsection.4.1}% 26
\BOOKMARK [3][-]{subsubsection.4.1.7}{Hardness\040aware\040Deep\040Metric\040Learning}{subsection.4.1}% 27
\BOOKMARK [3][-]{subsubsection.4.1.8}{Auto-Deeplab:\040NAS\040for\040Semantic\040Image\040Segmentation}{subsection.4.1}% 28
\BOOKMARK [3][-]{subsubsection.4.1.9}{Learning\040Loss\040for\040Active\040Learning\040}{subsection.4.1}% 29
\BOOKMARK [3][-]{subsubsection.4.1.10}{Striking\040the\040Right\040Balance\040With\040Uncertainty}{subsection.4.1}% 30
\BOOKMARK [3][-]{subsubsection.4.1.11}{Auto\040Augment}{subsection.4.1}% 31
\BOOKMARK [3][-]{subsubsection.4.1.12}{Zero\040Shot}{subsection.4.1}% 32
\BOOKMARK [3][-]{subsubsection.4.1.13}{Zero-shot\040Task\040Transfer}{subsection.4.1}% 33
\BOOKMARK [3][-]{subsubsection.4.1.14}{C-MIL:\040sWeakly\040Supervised\040Object\040Detection}{subsection.4.1}% 34
\BOOKMARK [3][-]{subsubsection.4.1.15}{Weakly\040Supervised\040Learning\040of\040Instance\040Segmentation}{subsection.4.1}% 35
\BOOKMARK [3][-]{subsubsection.4.1.16}{Attention-Based\040Dropout\040Layer\040for\040Weakly\040Supervised\040Object\040Localization}{subsection.4.1}% 36
\BOOKMARK [3][-]{subsubsection.4.1.17}{Domain\040Generalization\040by\040Sol\040Jigsaw\040Puzzles}{subsection.4.1}% 37
\BOOKMARK [3][-]{subsubsection.4.1.18}{Transferable\040Prototypical\040Networks\040for\040Unsupervised\040Domain\040Adaptation}{subsection.4.1}% 38
\BOOKMARK [3][-]{subsubsection.4.1.19}{Blending-target\040Domain\040Adaptation}{subsection.4.1}% 39
\BOOKMARK [3][-]{subsubsection.4.1.20}{ELASTIC:\040Improving\040CNNs\040with\040dynamic\040Scaling\040Policies}{subsection.4.1}% 40
\BOOKMARK [3][-]{subsubsection.4.1.21}{ScratchDet:\040Training\040Single-Shot\040Object\040Detectors\040From\040Scratch}{subsection.4.1}% 41
\BOOKMARK [3][-]{subsubsection.4.1.22}{SFNet:\040Learning\040Object-aware\040Semantic\040Correspondence}{subsection.4.1}% 42
\BOOKMARK [3][-]{subsubsection.4.1.23}{Deep\040Metric\040Learning\040Beyond\040Binary\040Supervision}{subsection.4.1}% 43
\BOOKMARK [3][-]{subsubsection.4.1.24}{Learning\040to\040Cluster\040Faces\040on\040an\040Affinity\040Graph}{subsection.4.1}% 44
\BOOKMARK [3][-]{subsubsection.4.1.25}{C2AE:\040Class\040Conditioned\040Auto-Encoder\040for\040Open-Set}{subsection.4.1}% 45
\BOOKMARK [3][-]{subsubsection.4.1.26}{Samsung:\040Learning\040to\040Quantize\040Deep\040Networks\040by\040Optimizing}{subsection.4.1}% 46
\BOOKMARK [3][-]{subsubsection.4.1.27}{Transfer\040Learning\040for\040Semantic\040Segmentation\040via\040Hierarchical\040Region\040Selection}{subsection.4.1}% 47
\BOOKMARK [3][-]{subsubsection.4.1.28}{Unsupervised\040Learning\040of\040Dense\040Shape\040Correspondence}{subsection.4.1}% 48
\BOOKMARK [3][-]{subsubsection.4.1.29}{Unsupervised\040Visual\040Domain\040Adaptation:\040A\040Deep\040Max-Margin\040Gaussian\040Process\040Approach}{subsection.4.1}% 49
\BOOKMARK [3][-]{subsubsection.4.1.30}{Balanced\040Self-Paced\040Learning\040for\040Generative\040Adversarial\040Clustering\040Network}{subsection.4.1}% 50
\BOOKMARK [3][-]{subsubsection.4.1.31}{Parallel\040Optimal\040Transport\040GAN}{subsection.4.1}% 51
\BOOKMARK [1][-]{section.5}{Wednsday\040June\04019th:\040Main\040Conference\040Day\0402}{}% 52
\BOOKMARK [2][-]{subsection.5.1}{Photography\040Oral:\040\(Day2\)}{section.5}% 53
\BOOKMARK [3][-]{subsubsection.5.1.1}{Photon-Flooded\040Single-Photon\0403D\040Cameras}{subsection.5.1}% 54
\BOOKMARK [3][-]{subsubsection.5.1.2}{High\040Flux\040Passive\040Imaging\040with\040Single-Photon\040Sensors}{subsection.5.1}% 55
\BOOKMARK [3][-]{subsubsection.5.1.3}{Acoustic\040Non\040Line\040of\040sight\040Imaging}{subsection.5.1}% 56
\BOOKMARK [3][-]{subsubsection.5.1.4}{Steady-State\040Non\040Line\040of\040Sight\040Imaging}{subsection.5.1}% 57
\BOOKMARK [3][-]{subsubsection.5.1.5}{A\040Theory\040of\040Fermat\040Paths\040for\040Non\040line\040of\040sight\040shape\040reconstruction\040\(CVPR19\040best\040paper\)}{subsection.5.1}% 58
\BOOKMARK [3][-]{subsubsection.5.1.6}{Projector\040Photometric\040Compensation}{subsection.5.1}% 59
\BOOKMARK [3][-]{subsubsection.5.1.7}{Bringing\040a\040Blurry\040Frame\040Alive\040at\040High\040Frame-Rate\040With\040an\040Event\040Camera}{subsection.5.1}% 60
\BOOKMARK [3][-]{subsubsection.5.1.8}{Bringing\040Alive\040Blurred\040Moments}{subsection.5.1}% 61
\BOOKMARK [3][-]{subsubsection.5.1.9}{Learning\040to\040Synthesize\040Motion\040Blur}{subsection.5.1}% 62
\BOOKMARK [3][-]{subsubsection.5.1.10}{Underexposed\040Photo\040Enhancement\040Using\040Deep\040Illumination\040Estimation}{subsection.5.1}% 63
\BOOKMARK [3][-]{subsubsection.5.1.11}{Blind\040Visual\040Motif\040Removal\040From\040a\040Single\040Image}{subsection.5.1}% 64
\BOOKMARK [3][-]{subsubsection.5.1.12}{Non-Local\040Meets\040Global:\040an\040integrated\040Paradigm\040for\040Hyper-spectral\040Denoising}{subsection.5.1}% 65
\BOOKMARK [3][-]{subsubsection.5.1.13}{Neural\040Rendering\040in\040the\040wild}{subsection.5.1}% 66
\BOOKMARK [3][-]{subsubsection.5.1.14}{GeoNet:\040Deep\040Geodesic\040Networks\040for\040Point\040Cloud\040Analysis}{subsection.5.1}% 67
\BOOKMARK [1][-]{section.6}{Thursday\040June\04020th:\040Main\040Conference\040Day\0403}{}% 68
\BOOKMARK [2][-]{subsection.6.1}{Low-level\040&\040Optimization\040Oral:\040\(Day3\)}{section.6}% 69
\BOOKMARK [3][-]{subsubsection.6.1.1}{Unprocessing\040Images\040for\040Raw\040Denoising}{subsection.6.1}% 70
\BOOKMARK [3][-]{subsubsection.6.1.2}{Residual\040Network\040for\040Light\040field\040image\040super\040resolution}{subsection.6.1}% 71
\BOOKMARK [3][-]{subsubsection.6.1.3}{Modulating\040image\040restoration\040with\040Continual\040Levels\040via\040adaptive\040feature\040Modification\040Layers}{subsection.6.1}% 72
\BOOKMARK [3][-]{subsubsection.6.1.4}{Second-Order\040Attention\040Network\040for\040Single\040Image\040Super-Resolution}{subsection.6.1}% 73
\BOOKMARK [3][-]{subsubsection.6.1.5}{Devil\040is\040in\040the\040Edges:\040learning\040Semantic\040Boundaries\040from\040noisy\040annotation}{subsection.6.1}% 74
\BOOKMARK [3][-]{subsubsection.6.1.6}{Path-Invariant\040Map\040Networks}{subsection.6.1}% 75
\BOOKMARK [3][-]{subsubsection.6.1.7}{FilterReg:\040Robust\040and\040Efficient\040Point-set\040registration}{subsection.6.1}% 76
\BOOKMARK [3][-]{subsubsection.6.1.8}{Probabilistic\040Permutation\040Synchronization\040Using\040the\040Riemannian\040Structure\040of\040the\040Birkhoff\040Polytope}{subsection.6.1}% 77
\BOOKMARK [3][-]{subsubsection.6.1.9}{Lifting\040Vectorial\040Variation\040Problems:}{subsection.6.1}% 78
\BOOKMARK [3][-]{subsubsection.6.1.10}{A\040Sufficient\040Condition\040for\040Convergences\040of\040Adam\040and\040RMSProp}{subsection.6.1}% 79
\BOOKMARK [3][-]{subsubsection.6.1.11}{Guaranteed\040Matrix\040Completion\040Under\040Multiple\040Linear\040Transformation}{subsection.6.1}% 80
\BOOKMARK [3][-]{subsubsection.6.1.12}{MAP\040inference\040via\040Block-Coordinate\040Frank-Wolfe\040Algorithm}{subsection.6.1}% 81
\BOOKMARK [3][-]{subsubsection.6.1.13}{A\040convex\040relaxation\040for\040multigraph\040matching}{subsection.6.1}% 82
