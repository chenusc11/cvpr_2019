\subsection{CVPR Main Day 1}
Oral:\\
\subsubsection{GNN}
\begin{enumerate}
    \item Few Shot Learning: EGNN
    \item Few Shot Classification
\end{enumerate}
\subsubsection{Kervolutional Neural Network}
    1. Introduce nonlinearity in convolution\\
\subsubsection{Relu with high confident}
    1. Adversarial confidence enhanced training (ACET)\\
\subsubsection{Structural Sensitivity of DCN to the Directions of FOurier Basis Functions}
\subsubsection{neural rejuvenation}
\subsubsection{On the Structural Sensitivity of Deep Convolution}
\subsubsection{Hardness aware Deep Metric Learning}
\subsubsection{Auto-Deeplab: NAS for Semantic Image Segmentation}
\subsubsection{Learning Loss for Active Learning }
    1. Active Learning \\
\subsubsection{Striking the Right Balance With Uncertainty}
\subsubsection{Auto Augment}
\subsubsection{Zero Shot}
    1. Domain Loss \\
    2. Triplet Loss \\
    3. Semantic Loss \\
\subsubsection{Zero-shot Task Transfer}
    1. Regress the unknown zero-shot task \\
\subsubsection{C-MIL: sWeakly Supervised Object Detection}
    1. Solve non-convex loss function problem \\
\subsubsection{Weakly Supervised Learning of Instance Segmentation}
    1. Learning Displacements to Centroid (Find Instance) \\
    2. Learning Class Boundaries (Find Shape) \\
\subsubsection{Attention-Based Dropout Layer for Weakly Supervised Object Localization}
    1. Adversarial Erasing \\
    2. Spatial Attention Transformation \\
    3. Attention-based Dropout layer \\
    4. attention-based, efficient, State of the art localization accuracy \\
\subsubsection{Domain Generalization by Sol Jigsaw Puzzles}
    1. Recognition -> Jigsaw Puzzles \\
    2. Domain Generalization \\
    3. Multitask deep learning model \\
\subsubsection{Transferable Prototypical Networks for Unsupervised Domain Adaptation}
    1. Most methods are cascaded model \\
    2. Multitask into one network \\
    3. Supervised classification loss, class-level discrepancy loss, sample-level discrepancy loss \\
\subsubsection{Blending-target Domain Adaptation}
    1. Source-target domain discrimination, still lack of target domain area \\
    2. Adaptation among Meta-sub-targets \\
\subsubsection{ELASTIC: Improving CNNs with dynamic Scaling Policies}
    1. {\bf Problem:} CNN image scaling are handcrafted \\
    2. {\bf Solution:} CNN to learn dynamic scaling policies \\
    3. {\bf Result:} Consistent improvement \\
\subsubsection{ScratchDet: Training Single-Shot Object Detectors From Scratch}
    1. {\bf Problem:} High Computational cost on ImageNet, Learning bias from classification to detection, inconvenient to change the architecture of network \\
    2. {\bf Contribution:}  \\
        1. Batch Norm \\
        2. Replace 7x7 by stacking 3x3 3x3... \\
        3. BatchNorm in the backbone key for detection to train from scratch \\
\subsubsection{SFNet: Learning Object-aware Semantic Correspondence}
    1. {\bf Problem:} Lack of dataset semantic correspondences \\
    2. {\bf Solution:} 3.Loss functions: Mask consistency, smoothness, and ... \\
    3. {\bf Result:}  \\
\subsubsection{Deep Metric Learning Beyond Binary Supervision}
    1. {\bf Problem:} most metric learning are same or not (binary). Population pos and neg are unbalanced \\
    2. {\bf Solution:}  \\
        1. Log-ratio loss. \\
        2. Dense Triplet Sampling \\
    3. {\bf Result:} \\
        1. Three retrieval: surpass state of the art \\
\subsubsection{Learning to Cluster Faces on an Affinity Graph}
    1. {\bf Problem:} Clustering human faces, complex structure are difficult to use kmeans or spectral \\
    2. {\bf Solution:} Generate Proposal - GCN-Detection - GCN-Segmentation \\
    3. {\bf Result:} state of the art F-score \\
\subsubsection{C2AE: Class Conditioned Auto-Encoder for Open-Set}
    1. {\bf Problem:} Open set recognition \\
    2. {\bf Solution:}  \\
        1. Closed set training \\
        2. Open set training, Decoder \\
        3. Open-set Testing (k-Inference Algorithm) \\
    3. {\bf Results:} \\
        1. F-measure highest \\
            1. Quantitative Analysis \\
\subsubsection{Samsung: Learning to Quantize Deep Networks by Optimizing}
    1. {\bf Problem:} Reducing bit-widths while minimizing accuracy drop \\
    2. {\bf Solution:} Find meaningful dynamic range for quantization \\
        1. Activation Quantizer \\
        2. Weight Quantizer \\
    3. {\bf Result:}  \\
        1. better than others \\
        2. Heterogeneous training \\
\subsubsection{Transfer Learning for Semantic Segmentation via Hierarchical Region Selection}
    1. {\bf Problem:} \\
        1. Insufficient real data + a lot of unreal data \\
    2. {\bf Solution:} \\
        1. Source Image with Weighting Mask \\
        2. Feed in Source \& target domain \\
        3. Use GAN to reduce domain differences \\
\subsubsection{Unsupervised Learning of Dense Shape Correspondence}
    1. {\bf Problem:} traditional supervised, want unsupervised \\
    2. {\bf Solutions:} \\
        1. No expensive annotations \\
        2. Geometric Invariants \\
    3. {\bf Result:} \\
        1. Achieve same result with Supervised \\
        2. Achieve state of the art accuracy without seen label \\
    4. Self-supervised Training Regime \\
\subsubsection{Unsupervised Visual Domain Adaptation: A Deep Max-Margin Gaussian Process Approach}
    1. {\bf Problem:} Traditional, learning source \& target distributions. Fail at lack of domain labels -> No guarantee \\
    2. {\bf Solution:} \\
        1. Source-driven Gaussian Process posterior (H) inference \\
        2. Target domain Max Margin Seperation \\
\subsubsection{Balanced Self-Paced Learning for Generative Adversarial Clustering Network}
    1. {\bf Problem:}  \\
    2. {\bf Solution:} Deep GAN Clustering Network \\
        1. Entropy Minimization Loss \\
\subsubsection{Parallel Optimal Transport GAN}
    1. {\bf Problem}
