\subsection{Low-level \& Optimization Oral: (Day3)}
\subsubsection{Unprocessing Images for Raw Denoising}
{\bf Problem:} Traditional: Synthetic Data Denoising. Not Real \\
        1. Real Camera Data: a high quality denoising dataset for smartphone cameras: best is BM3D \\
        2. SRGB image + additive Gaussian noise \\
        3. Raw + noise\\
{\bf Solution:} \\
        1. Find Unprocessed Data \\
        2. Raw sensor data $\ra$ Denise \& demosaic $\ra$ Color Correction $\ra$ gamma compression $\ra$ Tone mapping $\ra$ sRGB image \\
        3. Unprofessional images reverse the pipeline \\
        4. Realistic Training data\\
{\bf Results:} \\
        1. Best in DND\\
{\bf Takeaways:} \\
        1. Realistic Training data \\
        2. Unprocessing data $\ra$ better training results \\
\subsubsection{Residual Network for Light field image super resolution}
    1. {\bf Problem:} \\
        1. 
    2. {\bf Solution:} \\
        1. Extract subpixel in four direction \\
        2. Combine with ... \\
    3. {\bf Result:} \\
        1. Best result in Buddha and Mona \\
        2. Surpass EDSR \\
\subsubsection{Modulating image restoration with Continual Levels via adaptive feature Modification Layers}
    1. {\bf Problem:} \\
        1. Degradation levels of real world image are continuous \\
        2. Deep restoration is discontinuous \\
        3. Cannot train large model to handle all degradation levels \\
    2. {\bf Solution:} \\
        1. Propose AdaFM (Adaptive Feature Modification layer) \\
        2. 1. Train basic layer, 2. Insert AdaFM, 3.... \\
        3. 7x7 filter achieve better then 5x5 3x3. \\
        4. Arbitrary Result for Style transfer \\
\subsubsection{Second-Order Attention Network for Single Image Super-Resolution}
    1. {\bf Problem:} neglect rich feature correlations (most work) \\
    2. {\bf Solution:} \\
        1. Attention based mechanism \\
            1. Spatial Attention \\
            2. Channel Attention \\
        2. Use Spatial \& Channel attention simultaneously \\
            1. Second-order attention network SAN \\
            2. Use Newton Schulz iteration to solve eigenvalue decomposition (is not well supported on GPU platform) \\
    3. {\bf Result:} \\
        1. Better than VDSR \\
\subsubsection{Devil is in the Edges: learning Semantic Boundaries from noisy annotation}
    1. {\bf Problem:} \\
        1. Boundary annotation imprecise, current SOTA \\
        2. Annotation Hard \\
    2. {\bf Solution:} \\
        1. Propose STEAL \\
        2. Semantic Thinning Edge Alignment layer \\
    3. {\bf Result:} \\
        1. SBD dataset achieve state of the art \\
        2. CITYSCAPES: 4.2% better \\
    4. Dataset Refinement, annotate coarse masks fast, refine masks with STEAL \\
\subsubsection{Path-Invariant Map Networks}
    1. {\bf Problem:} \\
        1. Invariant map problem \\
    2. {\bf Solution:} \\
        1. Path Invariance provides a regularization for training neural networks \\
        2. Supervised loss + unsupervised loss \\
    3. {\bf Result:} \\
        1. Leverage additional training data \\
        2. Fuse attention... \\
        3. ... \\
\subsubsection{FilterReg: Robust and Efficient Point-set registration}
    1. {\bf Problem:} \\
        1. Iterative Closest Point: relatively fast but sensitive to.. \\
    2. {\bf Solution:} \\
        1. Filter-based Correspondence \\
    3. {\bf Result:}  \\
        1. 40ms runtime \\
        2. 30% faster than SOTA GPU method GMMTree \\
        3. Feature-based Registration \\
\subsubsection{Probabilistic Permutation Synchronization Using the Riemannian Structure of the Birkhoff Polytope}
    1. {\bf Problem:} \\
        1. Synchronization of Multiview Correspondences \\
        2. Correct Mistakes + Estimate Match Confidence \\
    2. {\bf Solution:} \\
        1. Encode Pairwise Correspondence\ as total permutations \\
        2. Minimize the cycle consistency loss over the entire (hyper-) graph \\
        3. Propose Birkhoff Riemannian Langevin Monte Carlo \\
    3. {\bf Result:} \\
        1. Achieve state of the art of L-BFGs algorithm \\
\subsubsection{Lifting Vectorial Variation Problems:}
    1. {\bf Problem:} \\
        1. Global energy minimization \\
    2. {\bf Solution:} \\
    3. OK I gave up \\
\subsubsection{A Sufficient Condition for Convergences of Adam and RMSProp}
    1. {\bf Problem:} \\
    2. {\bf Solution:} \\
        1. Modify Adam to Generic Adam \\
        2. Weighted AdaEMA \\
        3. Propose Sufficient condition \\
\subsubsection{Guaranteed Matrix Completion Under Multiple Linear Transformation}
    1. {\bf Problem:} \\
        1. Significant low-rank structure appears under some transformations \\
        2. The conventional theoretical analysis for guarantee is no longer suitable \\
    2. {\bf Solution:} \\
        1. Propose generalization the problem as Matrix Completion under Multi linear-Transformations MCMT \\
        2. The upper-bound of the reconstruction error is linearly controlled by the condition number of the transformations \\
\subsubsection{MAP inference via Block-Coordinate Frank-Wolfe Algorithm}
    1. {\bf Problem:} \\
\subsubsection{A convex relaxation for multigraph matching}
    1. {\bf Problem:} \\
        1. Multi-graph matching \\
        2. Cycle consistency matching \\
    2. {\bf Contribution:} \\
        1. Partial matching \\
        2. Quadratic costs \\
        3. Higher order costs \\
        4. Optimization: LP formulation \\
        5. Convergence: Lower bounds \& optimization gap \\
        6. Scalability: Linear \\
    3. {\bf Result} \\